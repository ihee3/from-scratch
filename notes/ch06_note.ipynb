{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr_grad[key]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum = 0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]\n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, lr, momentum_1=0.9, momentum_2=0.999):\n",
    "        self.lr = lr\n",
    "        self.momentum_1 = momentum_1\n",
    "        self.momentum_2 = momentum_2\n",
    "        self.v = None\n",
    "        self.s = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.v == None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.v[key] = (self.momentum_1*self.v[key] - self.lr*grads[key]) / (1 - self.momentum_1)\n",
    "        '''\n",
    "        v[key] = Momentum.update(params, grad)\n",
    "        v[key] = v[key] / (1-self.momentum_1)\n",
    "        '''\n",
    "\n",
    "        if self.s == None:\n",
    "            for key, val in params.items():\n",
    "                self.s[key] = np.zeros_like(val)\n",
    "\n",
    "        for key, val in params.items():\n",
    "            self.s[key] = (self.momentum_2 * self.s[key] - self.lr * (grads[key]**2) ) / (1 - self.momentum_2)\n",
    "            params[key] -= self.lr * self.v[key] / (np.sqrt(self.s[key]) + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_forward(x, gamma, beta, eps):\n",
    "    N, D = x.shape\n",
    "\n",
    "    mu = 1./N * np.sum(x, axis=0)\n",
    "    xmu = x - mu\n",
    "\n",
    "    sq = xmu ** 2\n",
    "    var = 1./N * np.sum(sq, axis=0)\n",
    "\n",
    "    sqrtvar = np.sqrt(var + eps)\n",
    "\n",
    "    ivar = 1./ivar\n",
    "\n",
    "    xhat = xmu * ivar\n",
    "\n",
    "    gammax = gamma * xhat\n",
    "\n",
    "    out = gammax + beta\n",
    "\n",
    "    cache = (xhat, gamma, xmu, ivar, sqrtvar, var, eps)\n",
    "\n",
    "    return out, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_backward(dout, cache):\n",
    "    xhat, gamma, xmu, ivar, sqrtvar, var, eps = cache\n",
    "\n",
    "    N, D = dout.shape\n",
    "\n",
    "    dbeta = np.sum(dout, axis=0)\n",
    "    dgammax = dout\n",
    "\n",
    "    dgamma = np.sum(dgammax*xhat, axis =0)\n",
    "    dxhat = dgamma * gamma\n",
    "\n",
    "    divar = np.sum(dxhat * xmu, axis =0)\n",
    "    dxmu_1 = dxhat * ivar\n",
    "\n",
    "    dsqrtvar = -1. / (sqrtvar**2) * divar\n",
    "    \n",
    "    dvar = 1. / np.sqrt(var + eps) * dsqrtvar\n",
    "\n",
    "    dsq = 1. /N *np.ones((N, D)) * dvar\n",
    "\n",
    "    dxmu_2 = 2 * xmu * dsq\n",
    "\n",
    "    dx1 = dxmu_1 + dxmu_2\n",
    "    dmu = -1 * np.sum(dxmu_1+dxmu_2, axis = 0)\n",
    "\n",
    "    dx2 = 1. / N * np.ones((N, D)) * dmu\n",
    "\n",
    "    dx = dx1 + dx2\n",
    "\n",
    "    return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio = 0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x, train_flag = True):\n",
    "        if train_flag:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (0.1 - self.dropout_ratio)\n",
    "            \n",
    "    def backward(self, dout):\n",
    "         return dout * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 16])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20])\n",
    "np.random.shuffle(x)\n",
    "x[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 10 ** np.random.uniform(-8, -4)\n",
    "lr = 10 ** np.random.uniform(-6, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001451801956583762"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7783568482033835e-06"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_trial = 10\n",
    "results_train = {}\n",
    "results_val = {}\n",
    "\n",
    "for _ in range(hyperparam_trial):\n",
    "    weight_decay = 10 ** np.random.uniform(-8, -4)\n",
    "    lr = 10 ** np.random.uniform(-6, -2)\n",
    "\n",
    "    val_acc_list, train_acc_list = _train(lr, weight_decay)\n",
    "    print('validation acc :' + str(val_acc_list[-1]) + \"| weight_decay : \" + str(weight_decay) + '| lr : ' + str(lr))\n",
    "    key = \"lr:\" + str(lr) + \", weight decay:\" + str(weight_decay)\n",
    "    results_val[key] = val_acc_list\n",
    "    results_train[key] = train_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
